{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance sampling using our compiler and BN translator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter's notebook shows our compiler which converts First-order-probabilistic-models (FOPPLs) using Lisp syntax to Bayesian Networks. Importance sampling is then done by sampling from the Bayesian networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **FOPPL → AST**  \n",
    "   - **Parsing:** Our code in `foppl_parser.py` handles tokenization via the `FOPPLLexer` class, then constructs an **Abstract Syntax Tree (AST)** using the `FOPPLParser` class.  \n",
    "   - **Why a dedicated parser?**  \n",
    "     - It cleanly separates the syntax from the semantics.  \n",
    "   - **Why AST Representation?** \n",
    "     - The resulting nodes (like `Sample`, `Observe`, `Let`) reside in `ast_nodes.py`. This AST is more convenient to traverse than raw Lisp. Also, it is easier to convert AST to a Bayesian Network (BN).\n",
    "\n",
    "2. **AST → Bayesian Network**  \n",
    "   - **Translation of AST to BN:** Next, in `translator.py`, we convert the AST to a BN in the form of a Python dictionary. \n",
    "   - **Why did we seperate the translator from the parser?**\n",
    "     - We did because then we can test the parser and translator independently before focusing on the inference part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "To execute importance sampling we need to be able to sample from our Bayesian Network graph, we do this as follows:\n",
    "1. We use our **`ImportanceSampler`** class (in `importance_sampling.py`) to draw samples from the network.  \n",
    "2. Each random node is sampled from its corresponding distribution, while observed nodes adjust the log-weight accordingly.  \n",
    "3. After drawing many samples and accumulating weights, we normalize and compute the posterior means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast_nodes import If, Constant, Sample, Let, Observe, Variable, Assign, For\n",
    "from translator import Translator\n",
    "from importance_sampling import ImportanceSampler\n",
    "import math\n",
    "from foppl_parser import FOPPLLexer, FOPPLParser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats  # for the Normal PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed AST:\n",
      "[Sample(x, ('normal', Constant(0.0), Constant(1.0))), Observe(('normal', Variable(x), Constant(2.0)), Constant(0.5))]\n",
      "BN graph:\n",
      "{'const_0': GraphNode(const_0, dist=None, obs=None, parents=[], det_expr=('const', 0.0)), 'const_1': GraphNode(const_1, dist=None, obs=None, parents=[], det_expr=('const', 1.0)), 'normal_2': GraphNode(normal_2, dist=None, obs=None, parents=['const_0', 'const_1'], det_expr=('dist', 'normal', ['const_0', 'const_1'])), 'x': GraphNode(x, dist=('normal_2',), obs=None, parents=['normal_2'], det_expr=None), 'const_3': GraphNode(const_3, dist=None, obs=None, parents=[], det_expr=('const', 2.0)), 'normal_4': GraphNode(normal_4, dist=None, obs=None, parents=['x', 'const_3'], det_expr=('dist', 'normal', ['x', 'const_3'])), 'obs_5': GraphNode(obs_5, dist=('normal_4',), obs=Constant(0.5), parents=['normal_4'], det_expr=None)}\n",
      "Approx. posterior mean of x = 0.114\n"
     ]
    }
   ],
   "source": [
    "# Test Importance Sampling\n",
    "\n",
    "## A simple AST:\n",
    "#    x ~ Normal(0,1)\n",
    "#    observe x ~ Normal(0,1) with obs=0.5\n",
    "# simple_ast_example = [\n",
    "#         Sample(\"x\", (\"normal\", 0.0, 1.0)),\n",
    "#         Observe((\"normal\", Variable(\"x\"), 2.0), 0.5)\n",
    "#     ]\n",
    "\n",
    "# A simple FOPPL program (Lisp-style syntax)\n",
    "simple_foppl = \"\"\"\n",
    "(sample x (normal 0.0 1.0))\n",
    "(observe (normal x 2.0) 0.5)\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Lexical analysis (tokenization)\n",
    "lexer = FOPPLLexer(simple_foppl)\n",
    "tokens = FOPPLParser(lexer)\n",
    "\n",
    "# Step 2: Parsing the tokens into an AST\n",
    "simple_ast = tokens.parse() \n",
    "print(f'Parsed AST:\\n{simple_ast}')\n",
    "\n",
    "# 2) Translate AST -> Graph\n",
    "translator = Translator()\n",
    "graph = translator.translate_program(simple_ast)\n",
    "print(f'BN graph:\\n{graph}')\n",
    "\n",
    "# 3) Instantiate ImportanceSampler\n",
    "sampler = ImportanceSampler(graph)\n",
    "\n",
    "# 4) Draw multiple samples\n",
    "N = 5000\n",
    "samples = []\n",
    "weights = []\n",
    "\n",
    "for _ in range(N):\n",
    "    env, log_weight = sampler.sample_one()\n",
    "    samples.append(env)\n",
    "    weights.append(math.exp(log_weight))  # store weight in linear scale\n",
    "\n",
    "# 5) Analyze results: e.g. compute weighted mean of x\n",
    "weighted_sum = 0.0\n",
    "total_weight = 0.0\n",
    "for env, w in zip(samples, weights):\n",
    "    weighted_sum += env[\"x\"] * w\n",
    "    total_weight += w\n",
    "posterior_mean_x = weighted_sum / total_weight\n",
    "\n",
    "print(f\"Approx. posterior mean of x = {posterior_mean_x:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL GRAPH NODES:\n",
      "\n",
      "const_0 -> GraphNode(const_0, dist=None, obs=None, parents=[], det_expr=('const', 0.0))\n",
      "const_1 -> GraphNode(const_1, dist=None, obs=None, parents=[], det_expr=('const', 1.0))\n",
      "normal_2 -> GraphNode(normal_2, dist=None, obs=None, parents=['const_0', 'const_1'], det_expr=('dist', 'normal', ['const_0', 'const_1']))\n",
      "x -> GraphNode(x, dist=('normal_2',), obs=None, parents=['normal_2'], det_expr=None)\n",
      "const_3 -> GraphNode(const_3, dist=None, obs=None, parents=[], det_expr=('const', 0.0))\n",
      "op_>_4 -> GraphNode(op_>_4, dist=None, obs=None, parents=['x', 'const_3'], det_expr=('>', 'x', 'const_3'))\n",
      "const_5 -> GraphNode(const_5, dist=None, obs=None, parents=[], det_expr=('const', 10.0))\n",
      "const_6 -> GraphNode(const_6, dist=None, obs=None, parents=[], det_expr=('const', -5.0))\n",
      "const_7 -> GraphNode(const_7, dist=None, obs=None, parents=[], det_expr=('const', -5.0))\n",
      "op_+_8 -> GraphNode(op_+_8, dist=None, obs=None, parents=['const_6', 'const_7'], det_expr=('+', 'const_6', 'const_7'))\n",
      "if_9 -> GraphNode(if_9, dist=None, obs=None, parents=['op_>_4', 'const_5', 'op_+_8'], det_expr=('if', 'op_>_4', 'const_5', 'op_+_8'))\n",
      "const_10 -> GraphNode(const_10, dist=None, obs=None, parents=[], det_expr=('const', 1.0))\n",
      "normal_11 -> GraphNode(normal_11, dist=None, obs=None, parents=['if_9', 'const_10'], det_expr=('dist', 'normal', ['if_9', 'const_10']))\n",
      "y -> GraphNode(y, dist=('normal_11',), obs=None, parents=['normal_11'], det_expr=None)\n",
      "const_12 -> GraphNode(const_12, dist=None, obs=None, parents=[], det_expr=('const', 1.0))\n",
      "normal_13 -> GraphNode(normal_13, dist=None, obs=None, parents=['y', 'const_12'], det_expr=('dist', 'normal', ['y', 'const_12']))\n",
      "obs_14 -> GraphNode(obs_14, dist=('normal_13',), obs=0.5, parents=['normal_13'], det_expr=None)\n"
     ]
    }
   ],
   "source": [
    "# Test BN translation\n",
    "from ast_nodes import If, Constant, Sample, Let, Observe, Variable, Assign, For\n",
    "from translator import Translator\n",
    "\n",
    "prog = [\n",
    "    Sample(\"x\", ('normal', 0.0, 1.0)),\n",
    "    Let(\"z\",\n",
    "        If(\n",
    "            # condition: x>0.0\n",
    "            ('>', Variable(\"x\"), Constant(0.0)),\n",
    "            Constant(10.0),\n",
    "            ('+', Constant(-5.0), Constant(-5.0))\n",
    "        ),\n",
    "        # body => sample y\n",
    "        Sample(\"y\", ('normal', Variable(\"z\"), 1.0))\n",
    "    ),\n",
    "    Observe(('normal', Variable(\"y\"), 1.0), 0.5)\n",
    "]\n",
    "\n",
    "# We'll define an initial \"acc\" = 0\n",
    "# prog = [\n",
    "#     Assign(\"acc\", Constant(0)),\n",
    "#     Sample(\"x\", (\"normal\", 0.0, 1.0)),\n",
    "#     For(\"i\", 3, [\n",
    "#         Assign(\"acc\", (\"+\", Variable(\"acc\"), Variable(\"i\")))\n",
    "#     ]),\n",
    "#     Observe((\"normal\", Variable(\"x\"), 1.0), 0.5),\n",
    "# ]\n",
    "\n",
    "translator = Translator()\n",
    "graph = translator.translate_program(prog)\n",
    "\n",
    "print(\"\\nFINAL GRAPH NODES:\\n\")\n",
    "for name, node in graph.items():\n",
    "    print(f\"{name} -> {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance sampling for model 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Unexpected token in expression: ('SYMBOL_RPAREN', ')') (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3460\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[12], line 19\u001b[0m\n    program_ast = tokens.parse()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:71\u001b[0m in \u001b[0;35mparse\u001b[0m\n    return self.program()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:77\u001b[0m in \u001b[0;35mprogram\u001b[0m\n    statements.append(self.statement())\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:93\u001b[0m in \u001b[0;35mstatement\u001b[0m\n    return self.parenthesized_expression()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:219\u001b[0m in \u001b[0;35mparenthesized_expression\u001b[0m\n    expr = self.statement()  # Parse the statement inside the parentheses\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:83\u001b[0m in \u001b[0;35mstatement\u001b[0m\n    return self.let_statement()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:122\u001b[0m in \u001b[0;35mlet_statement\u001b[0m\n    value_expr = self.expression()    # Let `expression()` produce an AST node\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:202\u001b[0m in \u001b[0;35mexpression\u001b[0m\n    return self.parenthesized_expression()  # Call to handle parenthesized sub-expressions\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:219\u001b[0m in \u001b[0;35mparenthesized_expression\u001b[0m\n    expr = self.statement()  # Parse the statement inside the parentheses\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:85\u001b[0m in \u001b[0;35mstatement\u001b[0m\n    return self.sample_statement()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:156\u001b[0m in \u001b[0;35msample_statement\u001b[0m\n    distribution_expr = self.distribution_expression()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:212\u001b[0m in \u001b[0;35mdistribution_expression\u001b[0m\n    arg2 = self.expression()\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\alexz\\OneDrive - Delft University of Technology\\Probalistic Models and Inference\\ProbabilisticInference6\\foppl_parser.py:204\u001b[1;36m in \u001b[1;35mexpression\u001b[1;36m\n\u001b[1;33m    raise SyntaxError(f\"Unexpected token in expression: {self.current_token}\")\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Unexpected token in expression: ('SYMBOL_RPAREN', ')')\n"
     ]
    }
   ],
   "source": [
    "mu0 = 50        # example prior mean\n",
    "tau2 = 10.0       # example prior variance\n",
    "y_obs = 45   # observed count\n",
    "\n",
    "# program_ast = [\n",
    "#     Sample(\"theta\", (\"normal\", mu0, tau2)),\n",
    "#     Observe((\"poisson\", Variable(\"theta\")), y_obs)\n",
    "# ]\n",
    "\n",
    "program =\"\"\"\n",
    "(let [theta (sample (normal mu0 tau2))]\n",
    "  (observe (poisson theta) y_obs)\n",
    "  theta)\n",
    "\"\"\"\n",
    "\n",
    "# Parse\n",
    "lexer = FOPPLLexer(program)\n",
    "tokens = FOPPLParser(lexer)\n",
    "program_ast = tokens.parse() \n",
    "print(f'Parsed AST:\\n{program_ast}')\n",
    "\n",
    "# Translate\n",
    "translator = Translator()\n",
    "graph = translator.translate_program(program_ast)\n",
    "sampler = ImportanceSampler(graph)\n",
    "\n",
    "N = 5000\n",
    "samples = []\n",
    "weights = []\n",
    "for i in range(N):\n",
    "    env, log_weight = sampler.sample_one()\n",
    "    samples.append(env)\n",
    "    weights.append(math.exp(log_weight))  # convert log weight to linear scale\n",
    "\n",
    "weighted_sum = 0.0\n",
    "total_weight = 0.0\n",
    "\n",
    "for env, w in zip(samples, weights):\n",
    "    weighted_sum += env[\"theta\"] * w\n",
    "    total_weight += w\n",
    "posterior_mean_theta = weighted_sum / total_weight\n",
    "\n",
    "theta_vals = np.array([env[\"theta\"] for env in samples])\n",
    "weights_array = np.array(weights)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the weighted histogram of posterior samples\n",
    "counts, bins, _ = plt.hist(theta_vals, bins=50, density=True, weights=weights_array,\n",
    "                         alpha=0.6, label='Weighted Posterior')\n",
    "\n",
    "sigma0 = math.sqrt(tau2)\n",
    "\n",
    "# Create a range of theta values for plotting the prior PDF\n",
    "theta_range = np.linspace(mu0 - 4*sigma0, mu0 + 4*sigma0, 500)\n",
    "\n",
    "# Compute the prior PDF for these theta values\n",
    "prior_pdf = stats.norm.pdf(theta_range, loc=mu0, scale=sigma0)\n",
    "\n",
    "# Plot the prior PDF\n",
    "plt.plot(theta_range, prior_pdf, 'r-', lw=2, label='Prior PDF')\n",
    "\n",
    "plt.xlabel('θ')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Posterior Distribution of θ with Prior Overlay')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Approx. posterior mean of theta = {posterior_mean_theta:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
