{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf3bf916-332d-485c-b1ee-c6790b81bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Distributions \n",
    "\n",
    "# abstract type Distribution{T} end\n",
    "# struct Half_Cauchy <: Distribution{Float64} end\n",
    "# const half_cauchy = Half_Cauchy()\n",
    "\n",
    "# function logpdf(::Half_Cauchy, x::Real, x0::Real, gamma::Real)\n",
    "#     if x<0\n",
    "#             x = x*-1\n",
    "#     end\n",
    "            \n",
    "#     return Distributions.logpdf(Distributions.Cauchy(x0, gamma), x)\n",
    "    \n",
    "# end\n",
    "\n",
    "# function logpdf_grad(::Half_Cauchy, x::Real, x0::Real, gamma::Real)\n",
    "#     if x<0\n",
    "#             x = x*-1\n",
    "#     end\n",
    "            \n",
    "#     x_x0 = x - x0\n",
    "#     x_x0_sq = x_x0^2\n",
    "#     gamma_sq = gamma^2\n",
    "#     deriv_x0 =  2 * x_x0 / (gamma_sq + x_x0_sq)\n",
    "#     deriv_x = - deriv_x0\n",
    "#     deriv_gamma = (x_x0_sq - gamma_sq) / (gamma * (gamma_sq + x_x0_sq))\n",
    "#     (deriv_x, deriv_x0, deriv_gamma)\n",
    "# end\n",
    "\n",
    "# is_discrete(::Half_Cauchy) = false\n",
    "\n",
    "# random(::Half_Cauchy, x0::Real, gamma::Real) = abs(rand(Distributions.Cauchy(x0, gamma)))\n",
    "\n",
    "# (::Half_Cauchy)(x0::Real, gamma::Real) = random(Half_Cauchy(), x0, gamma)\n",
    "\n",
    "# has_output_grad(::Half_Cauchy) = true\n",
    "# has_argument_grads(::Half_Cauchy) = (true, true)\n",
    "\n",
    "# export half_cauchy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "750c8be5-f4aa-465c-9db1-d9fd32907598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using  Gen\n",
    "\n",
    "# @gen function a()\n",
    "#     x ~ half_cauchy(0,1)\n",
    "\n",
    "# end\n",
    "\n",
    "# traces = [simulate(a, ()) for _ in 1:2]  # Run 100 importance samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc32744-13dd-4401-b7c4-bfa0d6cef7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final inferred mu: -0.03993062739567588\n",
      "Final inferred tau: -3.251773625600974\n",
      "Final inferred etas: [-0.39362237733627237, -0.10491281203139322, -1.5647180144604487, 0.20383097175604525, 1.5902634953952965, 0.2402140861756465, -0.12215540846954251, -0.869889725342703]\n"
     ]
    }
   ],
   "source": [
    "using Gen\n",
    "using Plots\n",
    "\n",
    "using Distributions \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#my_dist = print(rand(half_cauchy(0,25)))\n",
    "#my_dist = Truncated(Cauchy(), 0, Inf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the model with dynamic eta sampling\n",
    "@gen function simple_normal_model(sigma)\n",
    "    mu ~ normal(0, 1)     # Sample mu from a normal distribution\n",
    "    tau  ~ normal(2, 5)   # Sample tau from a Half-Cauchy distribution\n",
    "    \n",
    "    # Dynamic eta values sampled from normal distributions\n",
    "    list_of_Eta = [{(:eta, i)} ~ normal(0, 1) for i=1:length(sigma)]\n",
    "    \n",
    "    for i in 1:length(sigma)  # Loop over 5 iterations\n",
    "        # Calculate theta based on mu, tau, and eta\n",
    "        theta = mu + tau * list_of_Eta[i]\n",
    "        \n",
    "        # Sample obs from a normal distribution with mean theta and standard deviation sigma[i]\n",
    "         {(:y, i)} ~ normal(theta, sigma[i])\n",
    "    end\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "function do_inference(model, sigma, y_obs, num_iters, L, eps)\n",
    "    \n",
    "    # Create a choice map that maps model addresses (:y, i)\n",
    "    # to observed values ys[i]. We leave :slope and :intercept\n",
    "    # unconstrained, because we want them to be inferred.\n",
    "    observations = choicemap()\n",
    "    for (i, y) in enumerate(y_obs)\n",
    "        \n",
    "        observations[(:y, i)] = y\n",
    "    end\n",
    "    \n",
    "    # Call importance_resampling to obtain a likely trace consistent\n",
    "    # with our observations.\n",
    "    (trace, _) = generate(model, (sigma,), observations);\n",
    "    \n",
    "    accepted = 0\n",
    "  \n",
    "    # Metropolis–Hastings on :logtheta only (m0 and t2 are fixed)\n",
    "    for _ in 1:num_iters\n",
    "        # Run HMC with specified step size and trajectory length\n",
    "        (trace, accepted_this_iter) = hmc(trace, select(:mu, :tau, (:eta, i for i in 1:length(sigma))...), L=L, eps=eps)\n",
    "        accepted+= accepted_this_iter\n",
    "        \n",
    "    end\n",
    "\n",
    "    # Extract the final logtheta\n",
    "    final_choices = get_choices(trace)\n",
    "   \n",
    "    final_mu = final_choices[:mu]\n",
    "    final_tau = final_choices[:tau]\n",
    "    final_etas = [final_choices[(:eta, i)] for i in 1:length(sigma)]\n",
    "\n",
    "    acceptance_rate = accepted / num_iters  # Divide by total number of proposals\n",
    "\n",
    "    return (final_mu, final_tau, final_etas, acceptance_rate)\n",
    "end\n",
    "\n",
    "\n",
    "y_obs = [28, 8, -3, 7, -1, 1, 18, 12]\n",
    "sigma =[15, 10, 16, 11, 9, 11, 10, 18]\n",
    "\n",
    "\n",
    "# Use importance sampling as an example:\n",
    "# traces = [simulate(simple_normal_model, (sigma,)) for _ in 1:100]  # Run 100 importance samples\n",
    "\n",
    "# trace = do_inference(simple_normal_model, sigma, y_obs, 1000, 1, 0.1)\n",
    "\n",
    "## Run inference\n",
    "configurations = [\n",
    "    (eps=2, L=1),\n",
    "    (eps=2, L=2),\n",
    "    (eps=2, L=5),\n",
    "    (eps=2, L=10),\n",
    "    (eps=2, L=20)\n",
    "]\n",
    "\n",
    "\n",
    "# acceptance_rates = []\n",
    "# for (eps, L) in configurations\n",
    "#     final_mu, final_tau, final_etas, accepted = do_inference(simple_normal_model, sigma, y_obs, 5000, L, eps)\n",
    "#     println(\"Configuration: eps = $eps, L = $L\")\n",
    "#     println(\"Acceptance rate: $accepted\")\n",
    "#     #push!(acceptance_rates, (eps, L, acceptance_rate))\n",
    "# end\n",
    "\n",
    "final_mu, final_tau, final_etas, accepted = do_inference(simple_normal_model, sigma, y_obs, 5000, 1, 2)\n",
    "println(\"Final inferred mu: \", final_mu)\n",
    "println(\"Final inferred tau: \", final_tau)\n",
    "\n",
    "println(\"Final inferred etas: \", final_etas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c08f984-0bbc-41c2-84d0-f6a6102d976a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicDSLFunction{Any}(Dict{Symbol, Any}(), Dict{Symbol, Any}(), Type[Any, Any, Any, Any, Any, Any, Any, Any], false, Union{Nothing, Some{Any}}[nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing], Main.var\"##linear_regression_model#231\", Bool[0, 0, 0, 0, 0, 0, 0, 0], false)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Gen\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "@gen function linear_regression_model(X, y, N, K, sigma_alpha2, mu_beta, sigma_beta2, lambda_sigma)\n",
    "    # Sample the prior for intercept alpha and regression coefficients beta\n",
    "    alpha ~ normal(0, sqrt(sigma_alpha2))            # Intercept\n",
    "    beta = [{(:beta, i)} ~ normal(mu_beta, sqrt(sigma_beta2)) for i in 1:K]      # Regression coefficients\n",
    "    \n",
    "    # Sample degrees of freedom and scale for the Student's t-distribution\n",
    "    nu ~ gamma(2, 10)                                # Degrees of freedom for Student's t\n",
    "    sigma ~ exponential(lambda_sigma)                 # Scale for the Student's t\n",
    "\n",
    "    # Compute the mean for each y_i: μ_i = α + X_i * β\n",
    "    for i in 1:N\n",
    "       \n",
    "        mu_i = alpha + dot(X[i, :], beta)           # Linear model: μ_i = α + X_i * β\n",
    "        {(:y, i)} ~ student(mu_i, sigma)           # Sample y_i from Student's t distribution\n",
    "    end\n",
    "    \n",
    "    return :y  # Return the observed responses\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66dc0c2-d09c-46aa-839b-f85b8843f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final alpha: 0.3370987645106329\n",
      "Final betas: [0.2662668825476579, -1.144281211677809, 0.6915114681152962, -1.142215894993264, -0.9695292029160658]\n",
      "Final nu: 24.33062411187261\n",
      "Final sigma: 4.446274114584083\n",
      "Acceptance rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example run function for inference (adjusted to your example)\n",
    "function do_inference_linear_regression(X, y, N, K, sigma_alpha2, mu_beta, sigma_beta2, lambda_sigma, num_iters, L, eps)\n",
    "    # Set up observations as a choice map\n",
    "    observations = Gen.choicemap()\n",
    "    for (i, y) in enumerate(y)\n",
    "        observations[(:y, i)] = y\n",
    "    end\n",
    "\n",
    "    # Generate the model with initial data and priors\n",
    "    (trace, _) = generate(linear_regression_model, (X, y, N, K, sigma_alpha2, mu_beta, sigma_beta2, lambda_sigma), observations)\n",
    "    \n",
    "    accepted = 0\n",
    "  \n",
    "    # Metropolis-Hastings on the parameters with HMC\n",
    "    for _ in 1:num_iters\n",
    "        # Run HMC with specified step size and trajectory length\n",
    "      \n",
    "       (trace, accepted_this_iter) = hmc(trace, select(:alpha, :nu, :sigma,(:beta, i for i in 1:K)...), L=L, eps=eps)\n",
    "\n",
    "        accepted += accepted_this_iter\n",
    "    end\n",
    "\n",
    "    # Extract the final choices for parameters\n",
    "    final_choices = get_choices(trace)\n",
    "    final_alpha = final_choices[:alpha]\n",
    "    final_betas = [final_choices[(:beta, i)] for i in 1:K]\n",
    "    final_nu = final_choices[:nu]\n",
    "    final_sigma = final_choices[:sigma]\n",
    "\n",
    "    acceptance_rate = accepted / num_iters  # Divide by total number of proposals\n",
    "\n",
    "    return final_alpha, final_betas, final_nu, final_sigma, acceptance_rate\n",
    "end\n",
    "\n",
    "# Example of running inference with some random data\n",
    "N = 100  # Number of data points\n",
    "K = 5    # Number of features\n",
    "sigma_alpha2 = 1.0  # Prior variance for alpha\n",
    "mu_beta = 0.0  # Mean for beta prior\n",
    "sigma_beta2 = 1.0  # Variance for beta prior\n",
    "lambda_sigma = 1.0  # Rate for sigma prior\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "X = randn(N, K)  # N x K feature matrix\n",
    "true_alpha = 2.0\n",
    "true_beta = randn(K)  # True regression coefficients\n",
    "nu_true = 5.0\n",
    "sigma_true = 1.0\n",
    "\n",
    "# Linear model for generating synthetic y values\n",
    "mu = X * true_beta .+ true_alpha\n",
    "y = randn(N) .* sigma_true .+ mu  # Add noise to y values\n",
    "\n",
    "# Run inference\n",
    "final_alpha, final_betas, final_nu, final_sigma, acceptance_rate = do_inference_linear_regression(\n",
    "    X, y, N, K, sigma_alpha2, mu_beta, sigma_beta2, lambda_sigma, 5000, 5 , 0.001\n",
    ")\n",
    "\n",
    "println(\"Final alpha: \", final_alpha)\n",
    "println(\"Final betas: \", final_betas)\n",
    "println(\"Final nu: \", final_nu)\n",
    "println(\"Final sigma: \", final_sigma)\n",
    "println(\"Acceptance rate: \", acceptance_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
